<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>City of Wings - Projects</title><link href="https://city.shaform.com/en/" rel="alternate"></link><link href="https://city.shaform.com/feeds/projects.atom.xml" rel="self"></link><id>https://city.shaform.com/en/</id><updated>2015-11-06T21:20:00+08:00</updated><entry><title>snappycat: A command line tool to decompress snappy files produced by Hadoop</title><link href="https://city.shaform.com/en/blog/2015/11/06/snappycat.html" rel="alternate"></link><published>2015-11-06T21:20:00+08:00</published><updated>2015-11-06T21:20:00+08:00</updated><author><name>Shaform</name></author><id>tag:city.shaform.com,2015-11-06:/en/blog/2015/11/06/snappycat.html</id><summary type="html">&lt;p&gt;I often encounter [Snappy][]-compressed files recently when I am learning Spark.
Although we could just use &lt;code&gt;sc.textFile&lt;/code&gt; to read them in Spark, sometimes we
might want to download them locally for processing. However, reading these files
locally is complicated because the file format is not exactly Snappy-compressed files â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I often encounter [Snappy][]-compressed files recently when I am learning Spark.
Although we could just use &lt;code&gt;sc.textFile&lt;/code&gt; to read them in Spark, sometimes we
might want to download them locally for processing. However, reading these files
locally is complicated because the file format is not exactly Snappy-compressed files,
        as Hadoop stores those files in its own way.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    Most of existing solutions use Java to link with Hadoop library, but the setup is
    rather complicated. Moreover, some tools don&amp;#39;t support empty files. Therefore, I
    spent some time to study the file format.

    [Snappy]: https://blog.cloudera.com/blog/2011/09/snappy-and-hadoop/

    In short, Hadoop split the files into multiple blocks, and each block is compressed with
    Snappy independently. Before each compressed block, two 32-bit number are used to represent
    the decompressed size and the compressed size, respectively.

    As Spark split files into multiple partitions, some partitions might be empty. In such cases,
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the files would only contain two 32-bit 0s.&lt;/p&gt;
&lt;p&gt;I developed a short C++ program to handle these cases: &lt;a href="https://github.com/shaform/snappycat"&gt;snappycat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The usage is simple, just use input files as arguments:&lt;/p&gt;
&lt;div class="sh-highlight"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="hll"&gt;./snappycat DIRECTORY/*.snappy
&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;
&lt;p&gt;It also supports standard input:&lt;/p&gt;
&lt;div class="sh-highlight"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="hll"&gt;cat DIRECTARY/*.snappy &lt;span class="p"&gt;|&lt;/span&gt; snappycat
&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;
&lt;p&gt;The program outputs the decompressed result to standard output. So to save the output, use:&lt;/p&gt;
&lt;div class="sh-highlight"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="hll"&gt;./snappycat DIRECTORY/*.snappy &amp;gt; output.txt
&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;</content><category term="Hadoop"></category><category term="Spark"></category><category term="snappy"></category><category term="snappycat"></category></entry></feed>