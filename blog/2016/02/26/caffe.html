<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="前陣子有機會研究了一下 Caffe framework 的程式碼，於是把內容記下來避免忘記。 由於自己對該程式庫也不是很熟悉，加上一直拖稿導致 Caffe 在撰寫期間也已經有不少變動， 以致可能有許多不正確的地方，還請各位大大不吝指教。 主要分成四個部份來講。首先是整個 Caffe 的大架構，以及一些重要的元件。 其次，我也研究了如何自己新增一個 layer。 接下來，再重新回到...">
        <meta name="keywords" content="Caffe, deep learning">
        <link rel="icon" href="https://city.shaform.com/favicon.ico">

        <title>Caffe 程式閱讀筆記 - 翼之都, City of Wings</title>

        <!-- Stylesheets -->
        <link href="https://city.shaform.com/theme/css/all.min.css" rel="stylesheet">
        <link href="/style/style.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <link href="https://city.shaform.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="翼之都, City of Wings Full Atom Feed" />
        <link href="https://city.shaform.com/feeds/notes.atom.xml" type="application/atom+xml" rel="alternate" title="翼之都, City of Wings Categories Atom Feed" />
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->

        <!-- Google Analytics -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-13132274-1', 'auto');
          ga('send', 'pageview');
        </script>
        <!-- /Google Analytics -->

    </head>

    <body>

        <!-- Header -->
    <div class="header-container gradient">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="https://city.shaform.com/"><img class="mr20" src="https://city.shaform.com/images/fly.png" alt="logo">翼之都, City of Wings</a>
                    </div>
                    <div class="nav pull-right">
                            <a href="https://city.shaform.com/category/ideas.html">Ideas</a>
                            <a href="https://city.shaform.com/category/notes.html">Notes</a>
                            <a href="https://city.shaform.com/category/projects.html">Projects</a>
                            <a href="https://city.shaform.com/tags.html">Tags</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">Caffe 程式閱讀筆記</h1>
                      <p class="header-date">By <a href="https://city.shaform.com/author/shaform.html">Shaform</a>, Fri 26 February 2016, in category <a href="https://city.shaform.com/category/notes.html">Notes</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="https://city.shaform.com/tag/caffe.html">Caffe</a>, <a href="https://city.shaform.com/tag/deep-learning.html">deep learning</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <p>前陣子有機會研究了一下 Caffe framework 的程式碼，於是把內容記下來避免忘記。
由於自己對該程式庫也不是很熟悉，加上一直拖稿導致 Caffe 在撰寫期間也已經有不少變動，
以致可能有許多不正確的地方，還請各位大大不吝指教。</p>
<p>主要分成四個部份來講。首先是整個 Caffe 的大架構，以及一些重要的元件。
其次，我也研究了如何自己新增一個 layer。
接下來，再重新回到 Caffe 做更深入的解析。</p>
<iframe style="margin-left: auto; margin-right: auto; display: block;" src="https://docs.google.com/presentation/d/1jqVDrFxG9wJwovmH0XqP5wZZq1jBxRylHQRkQsz40tE/embed?start=false&loop=false&delayms=10000" frameborder="0" width="480" height="389" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

<h1>架構</h1>
<p>那麼，就從大架構開始講起。
Caffe 的 command line 工具有幾個功能，他可以讓你 train 一個 model，
也可以讓你用 train 好的 model 來進行效能的檢驗。當他在做 training 時，
他會建立一個 Solver 物件，他的主要功能就是協調類神經網路的運作來進行訓練。</p>
<p><img alt="Overview" src="https://city.shaform.com/images/caffe_overview.png"></p>
<p>我們可以用一個設定檔來指定 Solver 的參數，像是 learning rate 或者是 Solver 的種類，
例如 SGD Solver 等等。在設定檔中，可以指定一個 training net 的參數，
testing nets 則有可能有多個。例如如果要用不同的 data set 同時驗證 model 的效能時就可以用到。
雖然也可以直接把網路定義寫在 Solver 的設定檔裡，不過範例程式碼通常把他寫在獨立的設定檔中。</p>
<p>Solver 會根據這些設定檔，建立相對應的 training 和 testing 的 Net 物件。而 Net，
就會根據整個網路的定義，建立每個 Layer，同時也會建立很多的 Blobs 來放置 Layer
跟 Layer 間的輸出入資料，並把他們都接起來。其中，一個 layer 的輸入被稱為 bottom
blobs，輸出則為 top blobs。 Blob 基本上是一個多維陣列，不過他除了用來放 Data 外，
也同時包含一組對應的 Diff，可以用來放 Gradient 的計算結果。這些 Blobs 提供了簡易的界面，
可以讓 Layer 透過 GPU 或 CPU 來存取裡頭的資料。</p>
<p>而這些 Layers 除了有計算的功能以外，也有一些特別的 data layers 可以把資料從檔案中讀進來，
或者把輸出的結果寫到特定的檔案。此外，也有一些 loss layer 是用來計算最後預測結果的分數，
並藉此資訊讓 solver 得以最佳化所有的參數。每個 layer 會建立額外的 blobs 來放置這些可訓練的參數，
而 Net 在建立 layer 時，會把這些 blobs 也收集起來，方便 Solver 根據 learning rate 來計算每個參數的更新值。
當 Solver 呼叫了 Net 的 Forward 和 Backward 之後，資料就會沿著一層一層的 layer 進行計算。</p>
<h1>新增 Layer</h1>
<p>講完大略的架構後。我們就可以把焦點放在新增 layers 上。</p>
<p>要新增一個 layer，官方其實有提供<a href="https://github.com/BVLC/caffe/wiki/Development">簡單的指引</a>。
但除了單純的看文件之外，我們其實也可以參考看看以前的人是怎麼做的。</p>
<p>沒有錯，理論上應該有不同的人、在不同的時間點。分別新增了不同的  layers。
只要找出這些 commits 並且觀察裡頭的內容，應該就可以推測出要如何新增了。</p>
<p>所以，我選了兩個 pull requests:
<a href="https://github.com/BVLC/caffe/pull/1940">#1940</a>,
<a href="https://github.com/BVLC/caffe/pull/303">#303</a>，並且對照他們修改的檔案。</p>
<p>很快的，就可以看出其中的規律。他們都修改了一個叫做 caffe.proto 的檔案，
用來定義 layer 可以設定的參數。
HingeLoss 的修改還改動了 layer factory，不過看起來是因為以前在新增 layer 時，
要修改一個選擇 layer 的函數。
現在的 layer 都被放進一個 dictionary 裡，透過名稱取出，所以就不需要這種修改了。
只有一些有 cuDNN 實作的 layers 會在這裡放一個特別的函數來選擇實作的引擎。</p>
<p>接下來 <code>src/caffe/layers/*</code>, <code>include/caffe/*</code> 等檔案，則是 layer 實際的宣告以及實作。
以前 layer 的宣告依照分類被放置在不同的地方。
比如說 <code>neuron_layers.hpp</code> 通常是放進行 element-wise operations 的 layers。
<code>vision_layers.hpp</code> 則是放跟影像比較相關的 layers。
不過後來不同的 layers 就被搬到獨立的檔案了。</p>
<p>最後 <code>src/caffe/test/*</code> 則是一些測試。</p>
<p>於是我就實際的修改看看，我要加入一個很簡單的 layer，
他會把所有的輸入乘上一個事先指定的參數。
我在 caffe.proto 修改了三個地方，包含下個可用的 ID、放置參數的變數，
和實際的 layer 參數，也就是要乘上的那個常數。</p>
<div class="highlight"><pre><span></span><span class="c1">// src/caffe/proto/caffe.proto</span>
<span class="c1">// LayerParameter next available layer-specific ID: 144 (last added: zzz_param)</span>
<span class="n">message</span> <span class="n">LayerParameter</span> <span class="p">{</span>
  <span class="c1">// ...</span>
  <span class="n">optional</span> <span class="n">ZZZParameter</span> <span class="n">zzz_param</span> <span class="o">=</span> <span class="mi">143</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Message that stores parameters used by ZZZLayer</span>
<span class="n">message</span> <span class="n">ZZZParameter</span> <span class="p">{</span>
  <span class="c1">// Whether or not slope paramters are shared across channels.</span>
  <span class="n">optional</span> <span class="n">int32</span> <span class="n">mul</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mi">2</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>


<p>然後，我複製了一些簡單的宣告。其中主要的修改只有新增了一個 mul 參數，用來存放要乘的常數。</p>
<div class="highlight"><pre><span></span><span class="c1">// include/caffe/layers/zzz_layer.hpp</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span> <span class="nc">ZZZLayer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">NeuronLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">explicit</span> <span class="n">ZZZLayer</span><span class="p">(</span><span class="k">const</span> <span class="n">LayerParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">NeuronLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="p">{}</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">LayerSetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">);</span>

  <span class="k">virtual</span> <span class="kr">inline</span> <span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="nf">type</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="s">&quot;ZZZ&quot;</span><span class="p">;</span> <span class="p">}</span>

 <span class="k">protected</span><span class="o">:</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">Forward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">);</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">Forward_gpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">);</span>

  <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">);</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">Backward_gpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">);</span>
  <span class="n">Dtype</span> <span class="n">mul_</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>


<p>實作的部份因為初始化很簡單，只有讀取將來要乘的常數，所以主要的實作只有 Forward 和 Backward。
不過一些複雜的 layers 可能就會需要實作一些複雜的初始化，和計算輸出維度大小的函數。
Neuron layer 因為是 element-wise ，所以輸出的維度大小就跟輸入一樣。</p>
<p>Forward的部份，是先把 bottom_data 複製到 top，然後再將整個 top scale 指定的常數。</p>
<div class="highlight"><pre><span></span><span class="c1">// src/caffe/layers/zzz_layer.cpp</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">ZZZLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">bottom</span><span class="p">,</span>
                                     <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="n">caffe_copy</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">bottom_data</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">mul_</span> <span class="o">!=</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">caffe_scal</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">mul_</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>Backward 也很類似，確定需要計算後，先把 top_diff 複製到 bottom_diff 再 scale 指定的常數。</p>
<div class="highlight"><pre><span></span><span class="c1">// src/caffe/layers/zzz_layer.cpp</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">ZZZLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">top</span><span class="p">,</span>
                                      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">propagate_down</span><span class="p">,</span>
                                      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Dtype</span> <span class="o">*</span><span class="n">top_diff</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">();</span>
    <span class="n">Dtype</span> <span class="o">*</span><span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">();</span>
    <span class="n">caffe_copy</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">top_diff</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
    <span class="n">caffe_scal</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">mul_</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>CUDA 的版本也很類似，只是從 blob 要取出 gpu data ，然後 scale 時要選 GPU 的版本。</p>
<div class="highlight"><pre><span></span><span class="c1">// src/caffe/layers/zzz_layer.cu</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">ZZZLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_gpu</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">gpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_gpu_data</span><span class="p">();</span>
  <span class="n">caffe_copy</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">bottom_data</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">mul_</span> <span class="o">!=</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">caffe_gpu_scal</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">mul_</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">ZZZLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_gpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Dtype</span> <span class="o">*</span><span class="n">top_diff</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">gpu_diff</span><span class="p">();</span>
    <span class="n">Dtype</span> <span class="o">*</span><span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_gpu_diff</span><span class="p">();</span>
    <span class="n">caffe_copy</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">top_diff</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
    <span class="n">caffe_gpu_scal</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">mul_</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>最後我們再加一些 test，檢查 forward 確實成功乘上參數。</p>
<div class="highlight"><pre><span></span><span class="c1">// src/caffe/test/test_zzz_layer.cpp</span>
<span class="n">TYPED_TEST</span><span class="p">(</span><span class="n">NeuronLayerTest</span><span class="p">,</span> <span class="n">TestZZZForward</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">typedef</span> <span class="k">typename</span> <span class="n">TypeParam</span><span class="o">::</span><span class="n">Dtype</span> <span class="n">Dtype</span><span class="p">;</span>
  <span class="n">LayerParameter</span> <span class="n">layer_param</span><span class="p">;</span>
  <span class="n">ZZZLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="n">layer</span><span class="p">(</span><span class="n">layer_param</span><span class="p">);</span>
  <span class="n">layer</span><span class="p">.</span><span class="n">SetUp</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_bottom_vec_</span><span class="p">,</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_top_vec_</span><span class="p">);</span>
  <span class="n">layer</span><span class="p">.</span><span class="n">Forward</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_bottom_vec_</span><span class="p">,</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_top_vec_</span><span class="p">);</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_bottom_</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span>    <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_top_</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_bottom_</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">mul</span> <span class="o">=</span> <span class="n">layer_param</span><span class="p">.</span><span class="n">zzz_param</span><span class="p">().</span><span class="n">mul</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">EXPECT_FLOAT_EQ</span><span class="p">(</span><span class="n">top_data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">mul</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>同時也檢查用數值方法計算出來的 gradient 跟我們算的是否一樣。</p>
<div class="highlight"><pre><span></span><span class="c1">// src/caffe/test/test_zzz_layer.cpp</span>
<span class="n">TYPED_TEST</span><span class="p">(</span><span class="n">NeuronLayerTest</span><span class="p">,</span> <span class="n">TestZZZBackward</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">typedef</span> <span class="k">typename</span> <span class="n">TypeParam</span><span class="o">::</span><span class="n">Dtype</span> <span class="n">Dtype</span><span class="p">;</span>
  <span class="n">LayerParameter</span> <span class="n">layer_param</span><span class="p">;</span>
  <span class="n">ZZZLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="n">layer</span><span class="p">(</span><span class="n">layer_param</span><span class="p">);</span>
  <span class="n">GradientChecker</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="n">checker</span><span class="p">(</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mi">1701</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">);</span>
  <span class="n">checker</span><span class="p">.</span><span class="n">CheckGradientEltwise</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layer</span><span class="p">,</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_bottom_vec_</span><span class="p">,</span>
      <span class="k">this</span><span class="o">-&gt;</span><span class="n">blob_top_vec_</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>最後就可以實際測試看看。並看到測試的結果了。以上就是新增 layer 的部份。</p>
<div class="highlight"><pre><span></span>[----------] 2 tests from ZZZLayerTest/1, where TypeParam = caffe::CPUDevice&lt;double&gt;
[ RUN      ] ZZZLayerTest/1.TestZZZBackward
[       OK ] ZZZLayerTest/1.TestZZZBackward (2 ms)
[ RUN      ] ZZZLayerTest/1.TestZZZForward
[       OK ] ZZZLayerTest/1.TestZZZForward (0 ms)
[----------] 2 tests from ZZZLayerTest/1 (2 ms total)
</pre></div>


<h1>Dive into Caffe</h1>
<p>好，講完新增 layer 之後，這次可以更深入的研究 Caffe 的內部架構。</p>
<p>一開始我先到網路上搜尋了一下教學文件，包含官方的內容、課程的教學、或者是網友在問要怎麼讀 Caffe，以及各種網友的分享。最後再直接開始讀程式碼。</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe</a></li>
<li><a href="https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/preview">DIY Deep Learning for Vision</a></li>
<li><a href="http://vision.stanford.edu/teaching/cs231n/slides/caffe_tutorial.pdf">CS231n Caffe Tutorial</a></li>
<li><a href="http://www.zhihu.com/question/27982282">深度学习caffe的代码怎么读？</a></li>
<li><a href="https://yufeigan.github.io/tags/Caffe/">Caffe学习笔记</a></li>
<li><a href="http://www.cnblogs.com/nwpuxuezha/tag/caffe/">caffe笔记之例程学习</a></li>
<li><a href="https://www.quora.com/How-do-we-read-the-source-code-of-Caffe">How do we read the source code of Caffe?</a></li>
<li><a href="http://blog.csdn.net/kkk584520/article/category/2620633">Caffe代码导读</a></li>
</ul>
<p>剛剛已經講過 Caffe 的大致架構，不過實際上每個部件是怎麼互動的呢？
我們先從程式流程講起，之後再特別介紹 layers 的功能。</p>
<h2>Training</h2>
<p>以 training 的部份來說，程式實際上做的事大致如下：首先，載入參數，設定 GPU 等初始化。
接著，利用這些參數初始化一個 Solver 物件。接下來，如果是要載入 train 好的 weights
來 fine-tune 的話，就會直接把這些 weights 讀進 solver 裡頭的 Net 物件裡。
如果是要繼續進行到一半的 training 的話，則會把 snapshot 檔案當作參數傳給 Solver 的 solve。
否則就直接呼叫 Solve 開始 training 了。</p>
<p>而根據 Solver 種類的不同，會初始化一些不同的參數。但他們都會根據設定檔建立 training nets 以及 testing nets 等物件。</p>
<h2>Initialize Net</h2>
<p>每個 Net 被建立時會經過下列流程：</p>
<p>首先根據一些條件過濾掉參數中指定的 layers，比如說你可以把 testing 和 training
的 net 寫在一起，然後 input layer 分成兩個，一個只有在 training 時才有效，
另一個則在 testing 才有效，這樣在建立 Net 物件時依據不同階段，就會過濾掉不要的
input layer，達成載入不同的 data set 的目的。</p>
<p>接下來則是在參數中插入 split layers。
也就是在一些 output blob 被接到多個 layer 作為 input 的時候，
在中間多加入一個 split layer ,把 blob 複製成多份作為其他層的輸入。</p>
<p>這麼做大概有兩個目的，首先是收集從多個方向計算的 gradient ，
其次則是因為有些 layer 會做 in-place 的計算，也就是他 input 和 output 使用同一個 blob，
所以在這裡把 blob 分成多個，可以避免這種情形下的計算錯誤。</p>
<p>把 net 的參數經過之前的處理後，就會實際初始化 Net 物件。首先是建好網路最底層 input blobs。
再來則是按照順序一層一層的建立，並初始化每個 layers。</p>
<p>實際的流程是先建立 layer 物件後，先從之前的底層 input blobs 或者其他 layer 的
top blobs 中找到該 layer 的 bottom blob，也就是他的 input，然後呢，再視情況建立他的
top blob，也就是 output。</p>
<p>最後再用這些 input / output blobs 作為參數，完成 layer 的初始化。每個不同的
layer 會根據自己的需要分別實作不同的初始化程式。然後 net 再把該 layer 可以訓練的參數紀錄下來。</p>
<p>像這樣一層一層的初始化每個 layer 時，同時也找出哪些 layer 實際上需要 backward
的計算。這樣訓練時就可以節省不必要的計算。最後則是收集最後剩下來的 output blobs
，當作整個類神經網路的 output。同時也收集所有設定的 learning rate 和 weight
decay。</p>
<p>像這樣初始化完畢後，就進到真正的 trainning 了。</p>
<h2>Solver-&gt;Solve()</h2>
<p>在 training 時所作的，其實就是先做一次 forward 和 backward  的計算，再更新網路中的 weight，重複循環。計算 forward 的方法就是按照 layer 的順序，呼叫每個 layer 的 forward 函數。而 backward 就是用相反的順序呼叫每個 layer 的 backward 函數。每計算完一次就會得到更新的 gradients，然後就可以更新參數。</p>
<p>詳細流程是這樣的，收集完這些 gradients 後 Solver 會依照 learning rates 和 weight decay 算出每個參數應該更新的實際數字。然後再把有共用參數的 layer 的更新值加在一起，最後再一次更新所有的數值。</p>
<h2>Testing</h2>
<p>testing 時跟 training 也很類似，不過這次不需要建立 Solver ，而是直接建立 test net，並且讀取訓練好的 weight，最後執行 forward，就可以得到最後的輸出值了。</p>
<h1>Layers 簡介</h1>
<p>整個程式其實最重要的計算還是在 Layers 的部份。因此，我現在就來簡單介紹到底有哪些 layers 可供使用。</p>
<h2>Data Layers</h2>
<ul>
<li>DataLayer: 可以載入 leveldb 和 lmdb 的檔案。</li>
<li>DummyDataLayer: 用來產生一些亂數或其他預先定義的資料。</li>
<li>HDF5DataLayer: 可以讀取 HDF5 的檔案格式。</li>
<li>HDF5OutputLayer: 可以寫入 HDF5 的檔案格式。</li>
<li>ImageDataLayer: 直接載入圖片。</li>
<li>MemoryDataLayer: 可以透過程式，直接將記憶體中的資料放進 layer 中。</li>
<li>WindowDataLayer: 可以指定從一些圖片中擷取一些 windows 來當作輸入，比如說同一張圖裡幾個 windows 可能標記成貓。其他地方則有狗和沒有東西的標記。</li>
</ul>
<h2>Common Layers</h2>
<p>在 common layers 的分類中則有：</p>
<ul>
<li>ArgMaxLayer: 找出輸入中最大的 k 個值的 indices 或者 value。</li>
<li>ConcatLayer: 連接多個 bottom blobs。</li>
<li>EltwiseLayer: 他可以把多個 blobs 彼此 element-wise 進行加總，相乘，取最大等運算。</li>
<li>FlattenLayer: 把輸入的 blob 變成單維陣列。</li>
<li>InnerProductLayer: 其實就是輸出入全部接滿的 layers。</li>
<li>MVNLayer: 指的是 mean variance normalization，可以針對輸入做整體的 normalization。</li>
<li>SilenceLayer: 有點像垃圾桶，沒有輸出，只有輸入，被接到這裡的東西就會被丟掉，不會成為最後的輸出。</li>
<li>SplitLayer: 把輸出複製成多份。</li>
<li>SliceLayer: 把輸入切割成多分輸出。</li>
<li>SoftmaxLayer: 把一組輸出，轉換成機率輸出，也就是讓他們的和 normalize 成 1。</li>
</ul>
<h2>Neuron Layers</h2>
<p>neuron layers 主要是直接對每個元素運算。</p>
<ul>
<li>AbsValLayer: 取絕對值。</li>
<li>DropoutLayer: 隨機丟掉某些 input。</li>
<li>ExpLayer</li>
<li>PowerLayer</li>
<li>TanHLayer</li>
<li>ThresholdLayer: 若輸入大於門檻則為 1，否則為0。</li>
<li>BNLLLayer: 將輸入轉換成 binomial normal log likelihood。</li>
<li>ReLULayerReLu: 則是強制輸出不能小於零。</li>
<li>PReLULayerPReLU: 則多加了一個可以訓練的參數。接在後面。</li>
<li>SigmoidLayer: Sigmoid 可以把輸出限制在 0~1 之間，不過現在大家好像比較常用 ReLU 就是。</li>
</ul>
<h2>Vision Layers</h2>
<p>再來是 visions layers，其實這些 layers 都跟 CNN 有關。</p>
<ul>
<li>ConvolutionLayer: convolution 做的運算基本上是用數個 filter，或者說 feature detector，在原始輸入的局部小範圍中，進行一個內積運算，得到一個結果。平移這個小範圍做同樣的運算後，每個 filter 都可以得到一個個數比輸入稍微少一些的輸出。一般每個 feature detector 是用來偵測圖片中某些特定的特徵是否出現。</li>
<li>DeconvolutionLayer: 反過來，把一個輸入乘上一個矩陣。比如說右邊的 4 ，乘上一個 3x3 的矩陣，然後把他加回去左邊的輸出。</li>
<li>LRNLayer: LRN 則是一種 normalization 的方法，據論文上寫的，他似乎主要是用來 normalize 不同 filter 輸出的結果，讓訊號間彼此競爭，只留下一些勝利者。</li>
<li>PoolingLayer: 至於最後的 pooling ，則是選一塊輸入的範圍，進行取最大值或者平均的運算。得出個數較少的輸出。</li>
</ul>
<h1>結語</h1>
<p>感覺光是對一些運算做操作性的了解，似乎還是不能真正理解 Caffe 是如何被使用的。
未來應該要針對深度學習的理論上做更多研究才行。</p>


            <div class="comments">
                <div id="disqus_thread"></div>
                    <script type="text/javascript">
                        var disqus_shortname = 'cityofwings';
                        var disqus_identifier = 'blog/2016/02/26/caffe.html';
                        var disqus_url = 'https://city.shaform.com/blog/2016/02/26/caffe.html';
                        (function() {
                            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                        })();
                    </script>
                <noscript>Please enable JavaScript to view the comments.</noscript>
            </div>
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="https://city.shaform.com/archives.html">Archives</a></li>
                            <li><a href="https://city.shaform.com/tags.html">Tags</a></li>
                            <li><a href="https://city.shaform.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">Atom Feed</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Social</div>
                        <ul class="list-unstyled">
                            <li><a href="https://github.com/shaform" target="_blank">GitHub</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Links</div>
                        <ul class="list-unstyled">
                            <li><a href="https://shaform.com" target="_blank">About me</a></li>
                            <li><a href="https://shaform.wordpress.com" target="_blank">一座島</a></li>
                        </ul>
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; shaform 2015</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>